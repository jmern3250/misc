{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn \n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib \n",
    "import PIL \n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pdb\n",
    "\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape:  (391, 245, 437, 3)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN=1200\n",
    "# VALID=500\n",
    "TEST=391\n",
    "\n",
    "\n",
    "scene_dir='/home/jmern91/Documents/Coursework/CS231N/Project_Data_2/flyUpdate/scene/test'\n",
    "depth_dir='/home/jmern91/Documents/Coursework/CS231N/Project_Data_2/flyUpdate/depth/test'\n",
    "\n",
    "# X_train = np.zeros([TRAIN, 245, 437, 3])\n",
    "# Y_train = np.zeros([TRAIN, 245, 437, 3])\n",
    "# X_valid = np.zeros([VALID, 245, 437, 3])\n",
    "# Y_valid = np.zeros([VALID, 245, 437, 3])\n",
    "X_test = np.zeros([TEST, 245, 437, 3])\n",
    "Y_test = np.zeros([TEST, 245, 437, 3])\n",
    "\n",
    "# for i in range(TRAIN):\n",
    "#     idc = idx[i]\n",
    "#     x_path = scene_dir+str(idc)+'.jpg'\n",
    "#     y_path = depth_dir+str(idc)+'.jpg'\n",
    "# #     test = np.array(Image.open(x_path).convert('L'))\n",
    "# #     pdb.set_trace()\n",
    "#     X_train[i,:,:,:] = np.array(Image.open(x_path))/255.0\n",
    "#     Y_train[i,:,:,:] = np.array(Image.open(y_path))/255.0\n",
    "    \n",
    "# for i in range(VALID):\n",
    "#     idc = idx[i+TRAIN]\n",
    "#     x_path = scene_dir+str(idc)+'.jpg'\n",
    "#     y_path = depth_dir+str(idc)+'.jpg'\n",
    "#     X_valid[i,:,:,:] = np.array(Image.open(x_path))/255.0\n",
    "#     Y_valid[i,:,:,:] = np.array(Image.open(y_path))/255.0\n",
    "\n",
    "file_list = []\n",
    "i = 0\n",
    "for filename in glob.glob('/home/jmern91/Documents/Coursework/CS231N/Project_Data_2/flyUpdate/scene/test/*'): #assuming gif\n",
    "    im=Image.open(filename)\n",
    "    X_test[i,:,:,:] = np.array(im)/255.0\n",
    "    i += 1\n",
    "    file_list.append(filename)\n",
    "i = 0\n",
    "\n",
    "for filename in glob.glob('/home/jmern91/Documents/Coursework/CS231N/Project_Data_2/flyUpdate/depth/test/*'): #assuming gif\n",
    "    im=Image.open(filename)\n",
    "#     pdb.set_trace()\n",
    "    if np.array(im).ndim == 3:\n",
    "        Y_test[i,:,:,:] = np.array(im)/255.0\n",
    "    else:\n",
    "        Y_test[i,:,:,0] = np.array(im)/255.0\n",
    "        Y_test[i,:,:,1] = np.array(im)/255.0\n",
    "        Y_test[i,:,:,2] = np.array(im)/255.0\n",
    "    i += 1\n",
    "    \n",
    "# for i in range(TEST):\n",
    "#     idc = i\n",
    "#     x_path = scene_dir+str(idc)+'.jpg'\n",
    "#     y_path = depth_dir+str(idc)+'.jpg'\n",
    "#     X_test[i,:,:,:] = np.array(Image.open(x_path))/255.0\n",
    "#     Y_test[i,:,:,:] = np.array(Image.open(y_path))/255.0\n",
    "\n",
    "# print('Training Data Shape: ', X_train.shape)\n",
    "# print('Validation Data Shape: ', X_valid.shape)\n",
    "print('Test Data Shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, loss_val, Xd, Yd, \n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              L_rate=None, decay=None,\n",
    "              training=None, plot_losses=False,save=False):\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [loss_val]\n",
    "    if training_now:\n",
    "        variables.append(training)\n",
    "#     pdb.set_trace()\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses\n",
    "        losses = []\n",
    "        # Decay learning rate\n",
    "        if decay is not None: \n",
    "            L_rate *= decay\n",
    "        \n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         Y: Yd[idx,:],\n",
    "                         is_training: training_now,\n",
    "                         LR: L_rate}\n",
    "            # get batch size\n",
    "            actual_batch_size = Yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            if training_now:\n",
    "                loss, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            else: \n",
    "                loss = session.run(variables,feed_dict=feed_dict)\n",
    "            # aggregate performance stats\n",
    "#             pdb.set_trace()\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration %r: with minibatch training loss = %r \" % (iter_cnt,loss))\n",
    "            iter_cnt += 1\n",
    "#         pdb.set_trace()\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {1}, Overall loss = {0:.3g}\"\\\n",
    "              .format(total_loss,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_model(X, is_training):\n",
    "    conv0 = tf.layers.conv2d(\n",
    "            inputs=X,\n",
    "            filters = 8,\n",
    "            kernel_size = [4,4],\n",
    "            strides = 2,\n",
    "            padding = 'valid',\n",
    "            activation = tf.nn.relu,\n",
    "#                 name='conv0'\n",
    "    )\n",
    "\n",
    "    bn0 = tf.layers.batch_normalization(\n",
    "            conv0, training=is_training,\n",
    "#                 name='bn0'\n",
    "    )\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=bn0,\n",
    "            filters = 16,\n",
    "            kernel_size = [3,3],\n",
    "            strides = 2,\n",
    "            padding = 'valid',\n",
    "            activation = tf.nn.relu,\n",
    "#                 name='conv1'\n",
    "    )\n",
    "\n",
    "    bn1 = tf.layers.batch_normalization(\n",
    "            conv1, training=is_training,\n",
    "#                 name='bn1'\n",
    "    )\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=bn1,\n",
    "            filters = 16,\n",
    "            kernel_size = [3,3],\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = tf.nn.relu,\n",
    "#                 name='conv2'\n",
    "    )\n",
    "\n",
    "    bn2 = tf.layers.batch_normalization(\n",
    "            conv2, training=is_training,\n",
    "#                 name='bn2'\n",
    "    )\n",
    "\n",
    "    conv3 = tf.layers.conv2d(\n",
    "            inputs=bn2,\n",
    "            filters = 24,\n",
    "            kernel_size = [2,2],\n",
    "            strides = 1,\n",
    "            padding = 'valid',\n",
    "            activation = tf.nn.relu,\n",
    "#                 name='conv3'\n",
    "    )\n",
    "#     print(conv3.shape)\n",
    "    return conv3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_model(X, is_training):    \n",
    "    deconv4 = tf.layers.conv2d_transpose(\n",
    "            inputs=X,\n",
    "            filters = 64,\n",
    "            kernel_size=[6,6],\n",
    "            strides = 2,\n",
    "#             padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    bn3d = tf.layers.batch_normalization(\n",
    "            deconv4, training=is_training)\n",
    "    \n",
    "    deconv3 = tf.layers.conv2d_transpose(\n",
    "            inputs=bn3d,\n",
    "            filters = 32,\n",
    "            kernel_size = [6,6],\n",
    "            strides = 2,\n",
    "#             padding = 'same',\n",
    "            activation = tf.nn.relu)\n",
    "    bn2d = tf.layers.batch_normalization(\n",
    "            deconv3, training=is_training)\n",
    "    \n",
    "    deconv2 = tf.layers.conv2d_transpose(\n",
    "            inputs=bn2d,\n",
    "            filters = 16,\n",
    "            kernel_size = [5,5],\n",
    "            strides = 1,\n",
    "#             padding = 'same',\n",
    "            activation = tf.nn.relu)\n",
    "    bn1d = tf.layers.batch_normalization(\n",
    "            deconv2, training=is_training)\n",
    "    \n",
    "    deconv1 = tf.layers.conv2d_transpose(\n",
    "            inputs=bn1d,\n",
    "            filters = 4,\n",
    "            kernel_size = [5,5],\n",
    "            strides = 2,\n",
    "#             padding = 'same',\n",
    "            activation = tf.nn.relu)\n",
    "    bn0d = tf.layers.batch_normalization(\n",
    "            deconv1, training=is_training)\n",
    "#     print(deconv1.shape)\n",
    "    img_ = tf.layers.conv2d(\n",
    "            inputs=bn0d,\n",
    "            filters = 1,\n",
    "            kernel_size = [2,2],\n",
    "            strides = 2,\n",
    "#             padding = 'same',\n",
    "            activation = tf.nn.relu)\n",
    "    img = tf.squeeze(img_)\n",
    "#     print(img.shape)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 245, 437, 3])\n",
    "Y = tf.placeholder(tf.float32, [None, 245, 437, 3])\n",
    "LR = tf.placeholder(tf.float32)\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "with tf.variable_scope('Auto_Encoder')  as a_enc:\n",
    "    y_feats = encoder_model(Y, is_training)\n",
    "    \n",
    "with tf.variable_scope('Auto_Decoder') as a_dec:\n",
    "    y_img = decoder_model(y_feats, is_training)\n",
    "    \n",
    "with tf.variable_scope('Trans_Encoder')  as t_enc:\n",
    "    trans_feats_x = encoder_model(X, is_training)\n",
    "    \n",
    "# with tf.variable_scope(t_enc, reuse=True):\n",
    "#     trans_feats_y = encoder_model(Y, is_training)\n",
    "    \n",
    "with tf.variable_scope(a_dec,reuse=True):\n",
    "    trans_img = decoder_model(trans_feats_x, is_training)\n",
    "\n",
    "loss = tf.nn.l2_loss(y_img-tf.reduce_mean(Y,axis=3))\n",
    "mean_loss = tf.reduce_mean(loss) \n",
    "\n",
    "trans_loss = tf.nn.l2_loss(trans_feats_x - y_feats)\n",
    "trans_mean_loss = tf.reduce_mean(trans_loss) \n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LR)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "a_enc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='Auto_Encoder')\n",
    "a_dec_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='Auto_Decoder')\n",
    "t_enc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='Trans_Encoder')\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss, var_list=a_enc_vars+a_dec_vars)\n",
    "    train_step_trans = optimizer.minimize(trans_mean_loss, var_list=t_enc_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(var_list=t_enc_vars+a_enc_vars+a_dec_vars)\n",
    "saver.restore(sess, 'trans2_weights/trans2_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(TEST):\n",
    "    y = sess.run(trans_img,feed_dict={X: X_test[i:i+1,:,:,:], \n",
    "                                     is_training:False})\n",
    "#     pdb.set_trace()\n",
    "    result = Image.fromarray((y * 255.0).astype(np.uint8))\n",
    "    result.save(file_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = sess.run(trans_img,feed_dict={X: X_test[i:i+1,:,:,:], \n",
    "                                  is_training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
