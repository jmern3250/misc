{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import pdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_MLP(X,sizes,is_training,bits=8,dropout=False): \n",
    "    layers = {}\n",
    "    layers[0] = tf.layers.dense(X, sizes[0],\n",
    "                                activation = tf.nn.relu,\n",
    "                                use_bias=False,\n",
    "                                )\n",
    "    if dropout:\n",
    "        dropout_key = str(0)+'d' \n",
    "        layers[dropout_key] = tf.layers.dropout(layers[0],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "    for j,  size in enumerate(sizes[1:]): \n",
    "        i = j+1\n",
    "        if dropout: \n",
    "            dropout_key_prior = str(i-1)+'d' \n",
    "            layers[i] = tf.layers.dense(layers[dropout_key_prior], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "            dropout_key = str(i)+'d' \n",
    "            layers[dropout_key] = tf.layers.dropout(layers[dropout_key],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "        else:\n",
    "            layers[i] = tf.layers.dense(layers[i-1], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "\n",
    "    binary = tf.layers.dense(layers[len(sizes)-1], bits,\n",
    "                                   activation = tf.tanh,\n",
    "                                   use_bias=True,\n",
    "                                    ) \n",
    "    \n",
    "    binary += 1.0\n",
    "    binary *= 0.5\n",
    "    \n",
    "    output = tf.layers.dense(binary, 1,\n",
    "                            activation = None,\n",
    "                            use_bias=False,\n",
    "                                    ) \n",
    "    return [output, binary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "sizes = [512, 512, 512]\n",
    "output, binary = basic_MLP(X,sizes,is_training,bits=16,dropout=False)\n",
    "\n",
    "binary_loss = tf.reduce_mean((binary**3 - binary)**2)\n",
    "trans_loss = tf.reduce_mean((output - Y)**2)\n",
    "\n",
    "loss = trans_loss + 0.1*binary_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-2) \n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Loss: 0.0271673\n",
      "Translation Loss: 0.00150883\n",
      "Binary Loss: 0.0101796\n",
      "Translation Loss: 0.000181143\n",
      "Binary Loss: 0.0065316\n",
      "Translation Loss: 0.000212362\n",
      "Binary Loss: 0.00569265\n",
      "Translation Loss: 6.87164e-05\n",
      "Binary Loss: 0.00534498\n",
      "Translation Loss: 0.000160041\n",
      "Binary Loss: 0.00525592\n",
      "Translation Loss: 0.000426314\n",
      "Binary Loss: 0.00518281\n",
      "Translation Loss: 8.23801e-05\n",
      "Binary Loss: 0.00504495\n",
      "Translation Loss: 3.47351e-05\n",
      "Binary Loss: 0.00497606\n",
      "Translation Loss: 3.32131e-05\n",
      "Binary Loss: 0.00491436\n",
      "Translation Loss: 5.97514e-05\n"
     ]
    }
   ],
   "source": [
    "X_train = np.arange(-1.5, 1.5, 0.01)\n",
    "X_train = X_train.reshape([-1,1])\n",
    "feed_dict = {X:X_train, Y:X_train, is_training:True}\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    _, b_loss, t_loss = sess.run([train_step, binary_loss, trans_loss], feed_dict)\n",
    "    if (i+1)%100 == 0:\n",
    "        print('Binary Loss:', b_loss)\n",
    "        print('Translation Loss:', t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum error: 0.000829715\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    Xd = X_train[i,0].reshape([1,1])\n",
    "    feed_dict = {X:Xd, Y:Xd, is_training:False}\n",
    "    b_loss, t_loss = sess.run([binary_loss, trans_loss], feed_dict)\n",
    "    error.append(t_loss)\n",
    "max_error = np.amax(error)\n",
    "print('Maximum error:',max_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
