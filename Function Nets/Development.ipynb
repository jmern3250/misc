{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_code_table(bits=8, lo=-1.5, hi=1.5):\n",
    "    code_dict = {}\n",
    "    half_bins = int(2**bits/2)\n",
    "    prev_code_array = np.zeros([1,bits],dtype=np.dtype(int)).squeeze()\n",
    "    prev_code = ''.join(map(str, prev_code_array))\n",
    "    step = hi/half_bins\n",
    "    v_range = [0.0, step]\n",
    "    code_dict[prev_code] = v_range\n",
    "    for i in range(half_bins-1): \n",
    "        print('i:',i)\n",
    "        idx = bits-1\n",
    "        done = False\n",
    "        right_code = str()\n",
    "        while not done:\n",
    "            targ_bit = int(prev_code[idx])\n",
    "            targ_bit += 1\n",
    "            if targ_bit == 2:\n",
    "                right_code = str(0) + right_code \n",
    "                idx -= 1\n",
    "                done = False\n",
    "            else: \n",
    "                right_code = str(targ_bit) + right_code \n",
    "                done = True\n",
    "        code = prev_code[:idx] + right_code \n",
    "        v_range = [v_range[1], v_range[1]+step]\n",
    "        code_dict[code] = v_range\n",
    "        prev_code = code[:]\n",
    "    \n",
    "    \n",
    "    prev_code_array = np.zeros([1,bits],dtype=np.dtype(int)).squeeze()\n",
    "    prev_code_array[0] = 1\n",
    "    prev_code = ''.join(map(str, prev_code_array))\n",
    "    step = lo/half_bins\n",
    "    v_range = [0.0, step]\n",
    "    code_dict[prev_code] = v_range\n",
    "    for i in range(half_bins-1): \n",
    "        print('i:',i)\n",
    "        idx = bits-1\n",
    "        done = False\n",
    "        right_code = str()\n",
    "        while not done:\n",
    "            targ_bit = int(prev_code[idx])\n",
    "            targ_bit += 1\n",
    "            if targ_bit == 2:\n",
    "                right_code = str(0) + right_code \n",
    "                idx -= 1\n",
    "                done = False\n",
    "            else: \n",
    "                right_code = str(targ_bit) + right_code \n",
    "                done = True\n",
    "        code = prev_code[:idx] + right_code \n",
    "        v_range = [v_range[1], v_range[1]+step]\n",
    "        code_dict[code] = v_range\n",
    "        prev_code = code[:] \n",
    "        print(prev_code)\n",
    "    return code_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real2bits(value,bits=8,radius=1.5):\n",
    "    binary = np.zeros([bits,])\n",
    "    step = radius/(2**bits/2)\n",
    "    if value < 0:\n",
    "        binary[0] = 1\n",
    "    integer = int(np.floor(np.abs(value/step)))\n",
    "    done = False \n",
    "    i = bits-1\n",
    "    while not done: \n",
    "        binary[i] = integer % 2\n",
    "        integer = int(integer/2)\n",
    "#         pdb.set_trace()\n",
    "        if integer == 0:\n",
    "            done = True \n",
    "        else:\n",
    "            i -= 1 \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 1.5/2**(8-1)\n",
    "N = -1\n",
    "inp = N*step\n",
    "bit_array = real2bits(inp,bits=8,radius=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bits2real(bit_array,bits=8,radius=1.5):\n",
    "    step = radius/(2**bits/2)\n",
    "    weights = np.zeros([bits,])\n",
    "    weights[0] = -1.0\n",
    "    for i in range(bits-1):\n",
    "        weights[bits-i-1] = 2**i\n",
    "    value = weights.dot(bit_array)*step\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, output, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(output, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [loss_val,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            pdb.set_trace()\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_MLP(X,y,sizes,is_training,bits=8,threshold=0,dropout=False): \n",
    "    layers = {}\n",
    "    layers[0] = tf.layers.dense(X, sizes[0],\n",
    "                                activation = tf.nn.relu,\n",
    "                                use_bias=True,\n",
    "                                )\n",
    "    if dropout:\n",
    "        dropout_key = str(0)+'d' \n",
    "        layers[dropout_key] = tf.layers.dropout(layers[0],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "    for j,  size in enumerate(sizes[1:]): \n",
    "        i = j+1\n",
    "        if dropout: \n",
    "            dropout_key_prior = str(i-1)+'d' \n",
    "            layers[i] = tf.layers.dense(layers[dropout_key_prior], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "            dropout_key = str(i)+'d' \n",
    "            layers[dropout_key] = tf.layers.dropout(layers[dropout_key],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "        else:\n",
    "            layers[i] = tf.layers.dense(layers[i-1], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "\n",
    "    output_value = tf.layers.dense(layers[len(sizes)-1], bits,\n",
    "                                   activation = tf.nn.relu,\n",
    "                                   use_bias=False,\n",
    "                                    ) #used relu to guarentee no negative values\n",
    "    \n",
    "    output = tf.round(output_value)\n",
    "    \n",
    "#     threshold_list = [threshold]*bits\n",
    "#     threshold_tensor = tf.constant(threshold_list)\n",
    "#     output_bool = tf.greater_equal(output_value, threshold_tensor)\n",
    "#     output = tf.cast(output_bool,dtype=tf.uint8) \n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_bits2real(X, bits=8): #FIXME \n",
    "    weight_list = [-1]\n",
    "    for i in range(bits-1):\n",
    "        power = bits - 1 - i \n",
    "        weight_list.append(2**power)\n",
    "    weight_list = np.array(weight_list)\n",
    "    weight_list = weight_list.reshape([-1,1])\n",
    "    conversion_tensor = tf.constant(weight_list,\n",
    "                                   dtype=tf.float32)\n",
    "#     pdb.set_trace()\n",
    "    real_output = tf.matmul(X, conversion_tensor,\n",
    "                            transpose_a=False)\n",
    "    return real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"dense/kernel/read:0\", shape=(1, 64), dtype=float32)', 'Tensor(\"dense/bias/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"dense_1/kernel/read:0\", shape=(64, 128), dtype=float32)', 'Tensor(\"dense_1/bias/read:0\", shape=(128,), dtype=float32)', 'Tensor(\"dense_2/kernel/read:0\", shape=(128, 256), dtype=float32)', 'Tensor(\"dense_2/bias/read:0\", shape=(256,), dtype=float32)', 'Tensor(\"dense_3/kernel/read:0\", shape=(256, 8), dtype=float32)'] and loss Tensor(\"Sqrt:0\", shape=(1, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f6b74a99ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    284\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"dense/kernel/read:0\", shape=(1, 64), dtype=float32)', 'Tensor(\"dense/bias/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"dense_1/kernel/read:0\", shape=(64, 128), dtype=float32)', 'Tensor(\"dense_1/bias/read:0\", shape=(128,), dtype=float32)', 'Tensor(\"dense_2/kernel/read:0\", shape=(128, 256), dtype=float32)', 'Tensor(\"dense_2/bias/read:0\", shape=(256,), dtype=float32)', 'Tensor(\"dense_3/kernel/read:0\", shape=(256, 8), dtype=float32)'] and loss Tensor(\"Sqrt:0\", shape=(1, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "SIZES = [64, 128, 256]\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[1,1])\n",
    "y = tf.placeholder(tf.float32, shape=[1,1])\n",
    "is_training = tf.placeholder(tf.bool, shape=1)\n",
    "output = basic_MLP(X,y,SIZES,is_training,bits=8,threshold=0,dropout=False)\n",
    "real_output = tf_bits2real(output,bits=8)\n",
    "# pdb.set_trace()\n",
    "difference = real_output - y\n",
    "loss = tf.sqrt(tf.square(difference))\n",
    "optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import function\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def windowgrad(x,dy,center,width):\n",
    "#     lo = center - width/2.0\n",
    "#     hi = center + width/2.0\n",
    "#     lo = 1.0 - width/2.0\n",
    "#     hi = 1.0 + width/2.0\n",
    "# @function.Defun(tf.float32, tf.float32)\n",
    "# def windowgrad(x,dy):\n",
    "#     lo = tf.constant(0.5)\n",
    "#     hi = tf.constant(1.5)\n",
    "#     lo_mask = tf.greater_equal(x,lo)\n",
    "#     lo_mask = tf.cast(lo_mask,dtype=tf.float32)\n",
    "#     hi_mask = tf.less_equal(x,hi)\n",
    "#     hi_mask = tf.cast(hi_mask,dtype=tf.float32)\n",
    "#     grad = tf.constant(1.0, dtype=tf.float32,\n",
    "#                       shape=x.shape)\n",
    "#     grad = tf.multiply(grad, lo_mask)\n",
    "#     grad = tf.multiply(grad, hi_mask)\n",
    "#     grad = tf.multiply(grad,dy)\n",
    "#     return grad\n",
    "\n",
    "# @function.Defun(tf.float32, grad_func=windowgrad) # not needed because not a python function\n",
    "def window(x,center,width):\n",
    "    lo = center - width/2.0\n",
    "    hi = center + width/2.0\n",
    "#     lo = tf.constant(0.5)\n",
    "#     hi = tf.constant(1.5)\n",
    "    lo_mask = tf.greater_equal(x,lo)\n",
    "    lo_mask = tf.cast(lo_mask,dtype=tf.float32)\n",
    "    hi_mask = tf.less_equal(x,hi)\n",
    "    hi_mask = tf.cast(hi_mask,dtype=tf.float32)\n",
    "    out = tf.multiply(x, lo_mask)\n",
    "    out = tf.multiply(out, hi_mask)\n",
    "    return out \n",
    "\n",
    "# @ops.RegisterGradient(\"window\")\n",
    "# def _windowgrad(op, grad):\n",
    "# # def windowgrad(x,dy,center,width):\n",
    "# #     lo = center - width/2.0\n",
    "# #     hi = center + width/2.0\n",
    "# #     lo = 1.0 - width/2.0\n",
    "# #     hi = 1.0 + width/2.0\n",
    "#     x = op.inputs[0]\n",
    "#     lo = tf.constant(0.5)\n",
    "#     hi = tf.constant(1.5)\n",
    "#     lo_mask = tf.greater_equal(x,lo)\n",
    "#     hi_mask = tf.less_equal(x,hi)\n",
    "#     win_grad = tf.constant(1.0, dtype=tf.float32,\n",
    "#                       shape=x.shape)\n",
    "#     win_grad = tf.boolean_mask(win_grad, lo_mask)\n",
    "#     win_grad = tf.boolean_mask(win_grad, hi_mask)\n",
    "#     win_grad = tf.multiply(win_grad, grad)\n",
    "#     return [win_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_MLP_wcustom(X,y,sizes,is_training,bits=8,threshold=0,dropout=False): \n",
    "    layers = {}\n",
    "    layers[0] = tf.layers.dense(X, sizes[0],\n",
    "                                activation = tf.nn.relu,\n",
    "                                use_bias=True,\n",
    "                                )\n",
    "    if dropout:\n",
    "        dropout_key = str(0)+'d' \n",
    "        layers[dropout_key] = tf.layers.dropout(layers[0],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "    for j,  size in enumerate(sizes[1:]): \n",
    "        i = j+1\n",
    "        if dropout: \n",
    "            dropout_key_prior = str(i-1)+'d' \n",
    "            layers[i] = tf.layers.dense(layers[dropout_key_prior], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "            dropout_key = str(i)+'d' \n",
    "            layers[dropout_key] = tf.layers.dropout(layers[dropout_key],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "        else:\n",
    "            layers[i] = tf.layers.dense(layers[i-1], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "\n",
    "    output_value = tf.layers.dense(layers[len(sizes)-1], bits,\n",
    "                                   activation = None,\n",
    "                                   use_bias=False,\n",
    "                                    ) #used relu to guarentee no negative values\n",
    "#     pdb.set_trace()\n",
    "    center = tf.ones([1],dtype=tf.float32)\n",
    "    width = tf.constant(2.0,dtype=tf.float32)\n",
    "    output = window(output_value,center,width)\n",
    "#     output = output_value\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZES = [64, 128, 256]\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None,1],name='Input')\n",
    "y = tf.placeholder(tf.float32, shape=[None],name='Correct_output')\n",
    "is_training = tf.placeholder(tf.bool,name='training')\n",
    "output = basic_MLP_wcustom(X,y,SIZES,is_training,bits=8,threshold=0,dropout=False)\n",
    "real_output = tf_bits2real(output,bits=8)\n",
    "# pdb.set_trace()\n",
    "difference = real_output - y\n",
    "loss = tf.sqrt(tf.square(difference))\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "xd = np.arange(-1.49, 1.49, 0.001)\n",
    "# np.random.shuffle(xd)\n",
    "yd = np.zeros_like(xd)\n",
    "for i, x in enumerate(xd):\n",
    "    yd_ = real2bits(x, bits=8, radius=1.5)\n",
    "    yd[i] = bits2real(yd_, bits=8, radius=1.5)\n",
    "xd = xd.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    idx = np.random.randint(0,xd.shape[0]-1,100)\n",
    "    sess.run([train_step],feed_dict={X: xd[idx,:], y: yd[idx]})\n",
    "\n",
    "# total_loss, total_correct = run_model(sess, real_output, loss, xd, yd,\n",
    "#                                       epochs=1, batch_size=64, print_every=100,\n",
    "#                                       training=train_step, plot_losses=True)\n",
    "\n",
    "# iter_cnt,loss,np.sum(corr)/actual_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "correct = tf.equal(real_output, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "print(sess.run([accuracy],feed_dict={X: xd, y: yd}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 13.91115856],\n",
      "       [ 13.90182114],\n",
      "       [ 13.89248085],\n",
      "       ..., \n",
      "       [ 12.86473942],\n",
      "       [ 12.87338543],\n",
      "       [ 12.88204002]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run([real_output],feed_dict={X: xd, y: yd}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
