{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_code_table(bits=8, lo=-1.5, hi=1.5):\n",
    "    code_dict = {}\n",
    "    half_bins = int(2**bits/2)\n",
    "    prev_code_array = np.zeros([1,bits],dtype=np.dtype(int)).squeeze()\n",
    "    prev_code = ''.join(map(str, prev_code_array))\n",
    "    step = hi/half_bins\n",
    "    v_range = [0.0, step]\n",
    "    code_dict[prev_code] = v_range\n",
    "    for i in range(half_bins-1): \n",
    "        print('i:',i)\n",
    "        idx = bits-1\n",
    "        done = False\n",
    "        right_code = str()\n",
    "        while not done:\n",
    "            targ_bit = int(prev_code[idx])\n",
    "            targ_bit += 1\n",
    "            if targ_bit == 2:\n",
    "                right_code = str(0) + right_code \n",
    "                idx -= 1\n",
    "                done = False\n",
    "            else: \n",
    "                right_code = str(targ_bit) + right_code \n",
    "                done = True\n",
    "        code = prev_code[:idx] + right_code \n",
    "        v_range = [v_range[1], v_range[1]+step]\n",
    "        code_dict[code] = v_range\n",
    "        prev_code = code[:]\n",
    "    \n",
    "    \n",
    "    prev_code_array = np.zeros([1,bits],dtype=np.dtype(int)).squeeze()\n",
    "    prev_code_array[0] = 1\n",
    "    prev_code = ''.join(map(str, prev_code_array))\n",
    "    step = lo/half_bins\n",
    "    v_range = [0.0, step]\n",
    "    code_dict[prev_code] = v_range\n",
    "    for i in range(half_bins-1): \n",
    "        print('i:',i)\n",
    "        idx = bits-1\n",
    "        done = False\n",
    "        right_code = str()\n",
    "        while not done:\n",
    "            targ_bit = int(prev_code[idx])\n",
    "            targ_bit += 1\n",
    "            if targ_bit == 2:\n",
    "                right_code = str(0) + right_code \n",
    "                idx -= 1\n",
    "                done = False\n",
    "            else: \n",
    "                right_code = str(targ_bit) + right_code \n",
    "                done = True\n",
    "        code = prev_code[:idx] + right_code \n",
    "        v_range = [v_range[1], v_range[1]+step]\n",
    "        code_dict[code] = v_range\n",
    "        prev_code = code[:] \n",
    "        print(prev_code)\n",
    "    return code_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real2bits(value,bits=8,radius=1.5):\n",
    "    binary = np.zeros([bits,])\n",
    "    step = radius/(2**bits/2)\n",
    "    if value < 0:\n",
    "        binary[0] = 1\n",
    "    integer = int(np.floor(np.abs(value/step)))\n",
    "    done = False \n",
    "    i = bits-1\n",
    "    while not done: \n",
    "        binary[i] = integer % 2\n",
    "        integer = int(integer/2)\n",
    "#         pdb.set_trace()\n",
    "        if integer == 0:\n",
    "            done = True \n",
    "        else:\n",
    "            i -= 1 \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 1.5/2**(8-1)\n",
    "N = -1\n",
    "inp = N*step\n",
    "bit_array = real2bits(inp,bits=8,radius=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bits2real(bit_array,bits=8,radius=1.5):\n",
    "    step = radius/(2**bits/2)\n",
    "    weights = np.zeros([bits,])\n",
    "    weights[0] = -1.0\n",
    "    for i in range(bits-1):\n",
    "        weights[bits-i-1] = 2**i\n",
    "    value = weights.dot(bit_array)*step\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_MLP(X,y,sizes,is_training,bits=8,threshold=0,dropout=False): \n",
    "    layers = {}\n",
    "    layers[0] = tf.layers.dense(X, sizes[0],\n",
    "                                activation = tf.nn.relu,\n",
    "                                use_bias=True,\n",
    "                                )\n",
    "    if dropout:\n",
    "        dropout_key = str(0)+'d' \n",
    "        layers[dropout_key] = tf.layers.dropout(layers[0],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "    for j,  size in enumerate(sizes[1:]): \n",
    "        i = j+1\n",
    "        if dropout: \n",
    "            dropout_key_prior = str(i-1)+'d' \n",
    "            layers[i] = tf.layers.dense(layers[dropout_key_prior], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "            dropout_key = str(i)+'d' \n",
    "            layers[dropout_key] = tf.layers.dropout(layers[dropout_key],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "        else:\n",
    "            layers[i] = tf.layers.dense(layers[i-1], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "\n",
    "    output_value = tf.layers.dense(layers[len(sizes)-1], bits,\n",
    "                                   activation = tf.nn.relu,\n",
    "                                   use_bias=False,\n",
    "                                    ) #used relu to guarentee no negative values\n",
    "    \n",
    "    output = tf.round(output_value)\n",
    "    \n",
    "#     threshold_list = [threshold]*bits\n",
    "#     threshold_tensor = tf.constant(threshold_list)\n",
    "#     output_bool = tf.greater_equal(output_value, threshold_tensor)\n",
    "#     output = tf.cast(output_bool,dtype=tf.uint8) \n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_bits2real(X, bits=8):\n",
    "    weight_list = [-1]\n",
    "    for i in range(bits-1):\n",
    "        power = bits - 1 - i \n",
    "        weight_list.append(2**power)\n",
    "    weight_list = np.array(weight_list)\n",
    "    weight_list = weight_list.reshape([-1,1])\n",
    "    conversion_tensor = tf.constant(weight_list,\n",
    "                                   dtype=tf.float32)\n",
    "#     pdb.set_trace()\n",
    "    real_output = tf.matmul(X, conversion_tensor,\n",
    "                            transpose_a=False)\n",
    "    return real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'dense/kernel:0' shape=(1, 64) dtype=float32_ref>\", \"<tf.Variable 'dense/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'dense_1/kernel:0' shape=(64, 128) dtype=float32_ref>\", \"<tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'dense_2/kernel:0' shape=(128, 256) dtype=float32_ref>\", \"<tf.Variable 'dense_2/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'dense_3/kernel:0' shape=(256, 8) dtype=float32_ref>\"] and loss Tensor(\"Sqrt:0\", shape=(1, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4f6b74a99ff4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\johnm_000\\Anaconda2\\envs\\ipykernel_35\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    320\u001b[0m           \u001b[1;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m           \u001b[1;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'dense/kernel:0' shape=(1, 64) dtype=float32_ref>\", \"<tf.Variable 'dense/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'dense_1/kernel:0' shape=(64, 128) dtype=float32_ref>\", \"<tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'dense_2/kernel:0' shape=(128, 256) dtype=float32_ref>\", \"<tf.Variable 'dense_2/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'dense_3/kernel:0' shape=(256, 8) dtype=float32_ref>\"] and loss Tensor(\"Sqrt:0\", shape=(1, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "SIZES = [64, 128, 256]\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[1,1])\n",
    "y = tf.placeholder(tf.float32, shape=[1,1])\n",
    "is_training = tf.placeholder(tf.bool, shape=1)\n",
    "output = basic_MLP(X,y,SIZES,is_training,bits=8,threshold=0,dropout=False)\n",
    "real_output = tf_bits2real(output,bits=8)\n",
    "# pdb.set_trace()\n",
    "difference = real_output - y\n",
    "loss = tf.sqrt(tf.square(difference))\n",
    "optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_35]",
   "language": "python",
   "name": "conda-env-ipykernel_35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
