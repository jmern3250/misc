{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_code_table(bits=8, lo=-1.5, hi=1.5):\n",
    "    code_dict = {}\n",
    "    half_bins = int(2**bits/2)\n",
    "    prev_code_array = np.zeros([1,bits],dtype=np.dtype(int)).squeeze()\n",
    "    prev_code = ''.join(map(str, prev_code_array))\n",
    "    step = hi/half_bins\n",
    "    v_range = [0.0, step]\n",
    "    code_dict[prev_code] = v_range\n",
    "    for i in range(half_bins-1): \n",
    "        print('i:',i)\n",
    "        idx = bits-1\n",
    "        done = False\n",
    "        right_code = str()\n",
    "        while not done:\n",
    "            targ_bit = int(prev_code[idx])\n",
    "            targ_bit += 1\n",
    "            if targ_bit == 2:\n",
    "                right_code = str(0) + right_code \n",
    "                idx -= 1\n",
    "                done = False\n",
    "            else: \n",
    "                right_code = str(targ_bit) + right_code \n",
    "                done = True\n",
    "        code = prev_code[:idx] + right_code \n",
    "        v_range = [v_range[1], v_range[1]+step]\n",
    "        code_dict[code] = v_range\n",
    "        prev_code = code[:]\n",
    "    \n",
    "    \n",
    "    prev_code_array = np.zeros([1,bits],dtype=np.dtype(int)).squeeze()\n",
    "    prev_code_array[0] = 1\n",
    "    prev_code = ''.join(map(str, prev_code_array))\n",
    "    step = lo/half_bins\n",
    "    v_range = [0.0, step]\n",
    "    code_dict[prev_code] = v_range\n",
    "    for i in range(half_bins-1): \n",
    "        print('i:',i)\n",
    "        idx = bits-1\n",
    "        done = False\n",
    "        right_code = str()\n",
    "        while not done:\n",
    "            targ_bit = int(prev_code[idx])\n",
    "            targ_bit += 1\n",
    "            if targ_bit == 2:\n",
    "                right_code = str(0) + right_code \n",
    "                idx -= 1\n",
    "                done = False\n",
    "            else: \n",
    "                right_code = str(targ_bit) + right_code \n",
    "                done = True\n",
    "        code = prev_code[:idx] + right_code \n",
    "        v_range = [v_range[1], v_range[1]+step]\n",
    "        code_dict[code] = v_range\n",
    "        prev_code = code[:] \n",
    "        print(prev_code)\n",
    "    return code_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real2bits(value,bits=8,radius=1.5):\n",
    "    binary = np.zeros([bits,])\n",
    "    step = radius/(2**bits/2)\n",
    "    if value < 0:\n",
    "        binary[0] = 1\n",
    "    integer = int(np.floor(np.abs(value/step)))\n",
    "    done = False \n",
    "    i = bits-1\n",
    "    while not done: \n",
    "        binary[i] = integer % 2\n",
    "        integer = int(integer/2)\n",
    "#         pdb.set_trace()\n",
    "        if integer == 0:\n",
    "            done = True \n",
    "        else:\n",
    "            i -= 1 \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 1.5/2**(8-1)\n",
    "N = -1\n",
    "inp = N*step\n",
    "bit_array = real2bits(inp,bits=8,radius=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bits2real(bit_array,bits=8,radius=1.5):\n",
    "    step = radius/(2**(bits-1))\n",
    "    weights = np.zeros([bits,])\n",
    "#     weights[0] = -1.0\n",
    "    for i in range(bits-1):\n",
    "        weights[bits-i-1] = 2**i\n",
    "    value = weights.dot(bit_array)*step\n",
    "    if bool(bit_array[0]):\n",
    "        value *= -1.0\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, output, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(output, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [loss_val,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            pdb.set_trace()\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_MLP(X,y,sizes,is_training,bits=8,threshold=0,dropout=False): \n",
    "    layers = {}\n",
    "    layers[0] = tf.layers.dense(X, sizes[0],\n",
    "                                activation = tf.nn.relu,\n",
    "                                use_bias=True,\n",
    "                                )\n",
    "    if dropout:\n",
    "        dropout_key = str(0)+'d' \n",
    "        layers[dropout_key] = tf.layers.dropout(layers[0],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "    for j,  size in enumerate(sizes[1:]): \n",
    "        i = j+1\n",
    "        if dropout: \n",
    "            dropout_key_prior = str(i-1)+'d' \n",
    "            layers[i] = tf.layers.dense(layers[dropout_key_prior], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "            dropout_key = str(i)+'d' \n",
    "            layers[dropout_key] = tf.layers.dropout(layers[dropout_key],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "        else:\n",
    "            layers[i] = tf.layers.dense(layers[i-1], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "\n",
    "    output_value = tf.layers.dense(layers[len(sizes)-1], bits,\n",
    "                                   activation = tf.nn.relu,\n",
    "                                   use_bias=False,\n",
    "                                    ) #used relu to guarentee no negative values\n",
    "    \n",
    "    output = tf.round(output_value)\n",
    "    \n",
    "#     threshold_list = [threshold]*bits\n",
    "#     threshold_tensor = tf.constant(threshold_list)\n",
    "#     output_bool = tf.greater_equal(output_value, threshold_tensor)\n",
    "#     output = tf.cast(output_bool,dtype=tf.uint8) \n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_bits2real(X, bits=8,radius=1.5): #FIXME \n",
    "    step = radius/(2**(bits-1))\n",
    "    weight_list = [0]\n",
    "    for i in range(bits-1):\n",
    "        power = bits - 2 - i \n",
    "        weight_list.append(2**power)\n",
    "    weight_list = np.array(weight_list)\n",
    "    weight_list = weight_list.reshape([-1,1])\n",
    "#     pdb.set_trace()\n",
    "    conversion_tensor = tf.constant(weight_list,\n",
    "                                   dtype=tf.float32)\n",
    "#     pdb.set_trace()\n",
    "    real_output = tf.matmul(X, conversion_tensor,\n",
    "                            transpose_a=False)\n",
    "    idx_array = np.zeros([bits,1])\n",
    "    idx_array[0] = 1\n",
    "    idx_tensor = tf.constant(idx_array,\n",
    "                            dtype=tf.float32)\n",
    "    neg = tf.matmul(X,idx_tensor)\n",
    "    neg_factor = tf.multiply(neg, tf.constant(-2, dtype=tf.float32))\n",
    "    scale = neg_factor + 1.0\n",
    "    real_output = tf.multiply(real_output, scale)*step\n",
    "#     tf.cond(tf.cast(neg,tf.bool), )\n",
    "    return real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"dense/kernel/read:0\", shape=(1, 64), dtype=float32)', 'Tensor(\"dense/bias/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"dense_1/kernel/read:0\", shape=(64, 128), dtype=float32)', 'Tensor(\"dense_1/bias/read:0\", shape=(128,), dtype=float32)', 'Tensor(\"dense_2/kernel/read:0\", shape=(128, 256), dtype=float32)', 'Tensor(\"dense_2/bias/read:0\", shape=(256,), dtype=float32)', 'Tensor(\"dense_3/kernel/read:0\", shape=(256, 8), dtype=float32)'] and loss Tensor(\"Sqrt:0\", shape=(1, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f6b74a99ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    284\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"dense/kernel/read:0\", shape=(1, 64), dtype=float32)', 'Tensor(\"dense/bias/read:0\", shape=(64,), dtype=float32)', 'Tensor(\"dense_1/kernel/read:0\", shape=(64, 128), dtype=float32)', 'Tensor(\"dense_1/bias/read:0\", shape=(128,), dtype=float32)', 'Tensor(\"dense_2/kernel/read:0\", shape=(128, 256), dtype=float32)', 'Tensor(\"dense_2/bias/read:0\", shape=(256,), dtype=float32)', 'Tensor(\"dense_3/kernel/read:0\", shape=(256, 8), dtype=float32)'] and loss Tensor(\"Sqrt:0\", shape=(1, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "SIZES = [64, 128, 256]\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[1,1])\n",
    "y = tf.placeholder(tf.float32, shape=[1,1])\n",
    "is_training = tf.placeholder(tf.bool, shape=1)\n",
    "output = basic_MLP(X,y,SIZES,is_training,bits=8,threshold=0,dropout=False)\n",
    "real_output = tf_bits2real(output,bits=8)\n",
    "# pdb.set_trace()\n",
    "difference = real_output - y\n",
    "loss = tf.sqrt(tf.square(difference))\n",
    "optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/38833934/write-custom-python-based-gradient-function-for-an-operation-without-c-imple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-db442a02c344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import function\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def windowgrad(x,dy,center,width):\n",
    "#     lo = center - width/2.0\n",
    "#     hi = center + width/2.0\n",
    "#     lo = 1.0 - width/2.0\n",
    "#     hi = 1.0 + width/2.0\n",
    "# @function.Defun(tf.float32, tf.float32)\n",
    "# def windowgrad(x,dy):\n",
    "#     lo = tf.constant(0.5)\n",
    "#     hi = tf.constant(1.5)\n",
    "#     lo_mask = tf.greater_equal(x,lo)\n",
    "#     lo_mask = tf.cast(lo_mask,dtype=tf.float32)\n",
    "#     hi_mask = tf.less_equal(x,hi)\n",
    "#     hi_mask = tf.cast(hi_mask,dtype=tf.float32)\n",
    "#     grad = tf.constant(1.0, dtype=tf.float32,\n",
    "#                       shape=x.shape)\n",
    "#     grad = tf.multiply(grad, lo_mask)\n",
    "#     grad = tf.multiply(grad, hi_mask)\n",
    "#     grad = tf.multiply(grad,dy)\n",
    "#     return grad\n",
    "\n",
    "# @function.Defun(tf.float32, grad_func=windowgrad) # not needed because not a python function\n",
    "def window(x,center,width):\n",
    "    lo = center - width/2.0\n",
    "    hi = center + width/2.0\n",
    "#     lo = tf.constant(0.5)\n",
    "#     hi = tf.constant(1.5)\n",
    "    lo_mask = tf.greater_equal(x,lo)\n",
    "    lo_mask = tf.cast(lo_mask,dtype=tf.float32)\n",
    "    hi_mask = tf.less_equal(x,hi)\n",
    "    hi_mask = tf.cast(hi_mask,dtype=tf.float32)\n",
    "    out = tf.multiply(x, lo_mask)\n",
    "    out = tf.multiply(out, hi_mask)\n",
    "    return out \n",
    "\n",
    "# @ops.RegisterGradient(\"window\")\n",
    "# def _windowgrad(op, grad):\n",
    "# # def windowgrad(x,dy,center,width):\n",
    "# #     lo = center - width/2.0\n",
    "# #     hi = center + width/2.0\n",
    "# #     lo = 1.0 - width/2.0\n",
    "# #     hi = 1.0 + width/2.0\n",
    "#     x = op.inputs[0]\n",
    "#     lo = tf.constant(0.5)\n",
    "#     hi = tf.constant(1.5)\n",
    "#     lo_mask = tf.greater_equal(x,lo)\n",
    "#     hi_mask = tf.less_equal(x,hi)\n",
    "#     win_grad = tf.constant(1.0, dtype=tf.float32,\n",
    "#                       shape=x.shape)\n",
    "#     win_grad = tf.boolean_mask(win_grad, lo_mask)\n",
    "#     win_grad = tf.boolean_mask(win_grad, hi_mask)\n",
    "#     win_grad = tf.multiply(win_grad, grad)\n",
    "#     return [win_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_MLP_wcustom(X,y,sizes,is_training,bits=8,threshold=0,dropout=False): \n",
    "    layers = {}\n",
    "    layers[0] = tf.layers.dense(X, sizes[0],\n",
    "                                activation = tf.nn.relu,\n",
    "                                use_bias=True,\n",
    "                                )\n",
    "    if dropout:\n",
    "        dropout_key = str(0)+'d' \n",
    "        layers[dropout_key] = tf.layers.dropout(layers[0],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "    for j,  size in enumerate(sizes[1:]): \n",
    "        i = j+1\n",
    "        if dropout: \n",
    "            dropout_key_prior = str(i-1)+'d' \n",
    "            layers[i] = tf.layers.dense(layers[dropout_key_prior], size,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "            dropout_key = str(i)+'d' \n",
    "            layers[dropout_key] = tf.layers.dropout(layers[dropout_key],\n",
    "                                                rate=0.25,\n",
    "                                                training=is_training)\n",
    "        else:\n",
    "            layers[i] = tf.layers.dense(layers[i-1], size,\n",
    "                                    activation = tf.nn.tanh,\n",
    "                                    use_bias=True,\n",
    "                                    )\n",
    "\n",
    "    output_value = tf.layers.dense(layers[len(sizes)-1], bits,\n",
    "                                   activation = None,\n",
    "                                   use_bias=False,\n",
    "                                    ) #used relu to guarentee no negative values\n",
    "#     pdb.set_trace()\n",
    "    center = tf.ones([1],dtype=tf.float32)\n",
    "    width = tf.constant(1.0,dtype=tf.float32)\n",
    "#     output = window(output_value,center,width)\n",
    "    output = output_value\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SIZES = [64, 128, 256]\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None,1],name='Input')\n",
    "y = tf.placeholder(tf.float32, shape=[None],name='Correct_output')\n",
    "is_training = tf.placeholder(tf.bool,name='training')\n",
    "output = basic_MLP_wcustom(X,y,SIZES,is_training,bits=8,threshold=0,dropout=False)\n",
    "real_output = tf_bits2real(output,bits=8)\n",
    "# pdb.set_trace()\n",
    "difference = real_output - y\n",
    "loss = tf.square(difference)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-6) \n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "xd = np.arange(0.0, 1.49, 0.01)\n",
    "# np.random.shuffle(xd)\n",
    "yd = np.zeros_like(xd)\n",
    "for i, x in enumerate(xd):\n",
    "    yd_ = real2bits(x, bits=8, radius=1.5)\n",
    "    yd[i] = bits2real(yd_, bits=8, radius=1.5)\n",
    "#     pdb.set_trace()\n",
    "xd = xd.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167536\n",
      "0.19134\n",
      "0.182961\n",
      "0.200914\n",
      "0.166481\n",
      "0.18901\n",
      "0.18806\n",
      "0.165333\n",
      "0.183184\n",
      "0.19052\n",
      "0.188898\n",
      "0.182952\n",
      "0.161439\n",
      "0.168668\n",
      "0.180057\n",
      "0.176598\n",
      "0.199147\n",
      "0.157843\n",
      "0.198335\n",
      "0.19307\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    idx = np.random.randint(0,xd.shape[0]-1,100)\n",
    "    ml, _ = sess.run([mean_loss, train_step],feed_dict={X: xd[idx,:], y: yd[idx]})\n",
    "    if i % 1000 == 0:\n",
    "        print(ml)\n",
    "\n",
    "# total_loss, total_correct = run_model(sess, real_output, loss, xd, yd,\n",
    "#                                       epochs=1, batch_size=64, print_every=100,\n",
    "#                                       training=train_step, plot_losses=True)\n",
    "\n",
    "# iter_cnt,loss,np.sum(corr)/actual_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "correct = tf.equal(real_output, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "print(sess.run([accuracy],feed_dict={X: xd, y: yd}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.611404  ],\n",
      "       [ 0.61801881],\n",
      "       [ 0.62460148],\n",
      "       [ 0.63115221],\n",
      "       [ 0.63734317],\n",
      "       [ 0.64251858],\n",
      "       [ 0.64650023],\n",
      "       [ 0.65043199],\n",
      "       [ 0.65432096],\n",
      "       [ 0.65789437],\n",
      "       [ 0.66114408],\n",
      "       [ 0.66418988],\n",
      "       [ 0.66718888],\n",
      "       [ 0.67013872],\n",
      "       [ 0.67304146],\n",
      "       [ 0.67589653],\n",
      "       [ 0.67870331],\n",
      "       [ 0.68146312],\n",
      "       [ 0.68417531],\n",
      "       [ 0.68683958],\n",
      "       [ 0.68945658],\n",
      "       [ 0.69202626],\n",
      "       [ 0.69454849],\n",
      "       [ 0.69702345],\n",
      "       [ 0.69945109],\n",
      "       [ 0.70183206],\n",
      "       [ 0.70416552],\n",
      "       [ 0.70645285],\n",
      "       [ 0.70869273],\n",
      "       [ 0.710886  ],\n",
      "       [ 0.71303231],\n",
      "       [ 0.71513224],\n",
      "       [ 0.71718574],\n",
      "       [ 0.71919304],\n",
      "       [ 0.72115362],\n",
      "       [ 0.72306824],\n",
      "       [ 0.72493672],\n",
      "       [ 0.72675931],\n",
      "       [ 0.72853577],\n",
      "       [ 0.73026669],\n",
      "       [ 0.73195207],\n",
      "       [ 0.73359156],\n",
      "       [ 0.7351861 ],\n",
      "       [ 0.73673457],\n",
      "       [ 0.73823857],\n",
      "       [ 0.73969704],\n",
      "       [ 0.74111062],\n",
      "       [ 0.74247956],\n",
      "       [ 0.74380404],\n",
      "       [ 0.74508345],\n",
      "       [ 0.74631846],\n",
      "       [ 0.74750972],\n",
      "       [ 0.74865645],\n",
      "       [ 0.74975878],\n",
      "       [ 0.75081772],\n",
      "       [ 0.75183296],\n",
      "       [ 0.75280464],\n",
      "       [ 0.75373268],\n",
      "       [ 0.75461769],\n",
      "       [ 0.75545907],\n",
      "       [ 0.75625789],\n",
      "       [ 0.75701392],\n",
      "       [ 0.75772709],\n",
      "       [ 0.75839746],\n",
      "       [ 0.75902557],\n",
      "       [ 0.75961208],\n",
      "       [ 0.76015592],\n",
      "       [ 0.76065826],\n",
      "       [ 0.76111847],\n",
      "       [ 0.76153731],\n",
      "       [ 0.76191437],\n",
      "       [ 0.76225102],\n",
      "       [ 0.76254612],\n",
      "       [ 0.76280069],\n",
      "       [ 0.7630142 ],\n",
      "       [ 0.76318735],\n",
      "       [ 0.76332039],\n",
      "       [ 0.76341259],\n",
      "       [ 0.76346523],\n",
      "       [ 0.76347792],\n",
      "       [ 0.76345122],\n",
      "       [ 0.76338494],\n",
      "       [ 0.76327997],\n",
      "       [ 0.76313484],\n",
      "       [ 0.76295131],\n",
      "       [ 0.76272953],\n",
      "       [ 0.76246893],\n",
      "       [ 0.7621699 ],\n",
      "       [ 0.76183295],\n",
      "       [ 0.7614575 ],\n",
      "       [ 0.76104462],\n",
      "       [ 0.76059425],\n",
      "       [ 0.7601068 ],\n",
      "       [ 0.7595818 ],\n",
      "       [ 0.75902033],\n",
      "       [ 0.75842178],\n",
      "       [ 0.75778663],\n",
      "       [ 0.75711548],\n",
      "       [ 0.75640744],\n",
      "       [ 0.75566387],\n",
      "       [ 0.75488466],\n",
      "       [ 0.75406981],\n",
      "       [ 0.75321954],\n",
      "       [ 0.75233442],\n",
      "       [ 0.75141424],\n",
      "       [ 0.75045907],\n",
      "       [ 0.74946958],\n",
      "       [ 0.74844587],\n",
      "       [ 0.74738771],\n",
      "       [ 0.74629617],\n",
      "       [ 0.7451703 ],\n",
      "       [ 0.74401152],\n",
      "       [ 0.74281913],\n",
      "       [ 0.74159408],\n",
      "       [ 0.7403357 ],\n",
      "       [ 0.73904443],\n",
      "       [ 0.73772156],\n",
      "       [ 0.73636621],\n",
      "       [ 0.73497832],\n",
      "       [ 0.73355925],\n",
      "       [ 0.73210841],\n",
      "       [ 0.73062599],\n",
      "       [ 0.72911298],\n",
      "       [ 0.72756863],\n",
      "       [ 0.72599387],\n",
      "       [ 0.724388  ],\n",
      "       [ 0.72275233],\n",
      "       [ 0.72108662],\n",
      "       [ 0.71939069],\n",
      "       [ 0.71766531],\n",
      "       [ 0.71591043],\n",
      "       [ 0.71412623],\n",
      "       [ 0.712313  ],\n",
      "       [ 0.71047133],\n",
      "       [ 0.70860076],\n",
      "       [ 0.70670211],\n",
      "       [ 0.70477486],\n",
      "       [ 0.70281965],\n",
      "       [ 0.70083749],\n",
      "       [ 0.69882715],\n",
      "       [ 0.69679022],\n",
      "       [ 0.69472545],\n",
      "       [ 0.69263411],\n",
      "       [ 0.69051641],\n",
      "       [ 0.68837154],\n",
      "       [ 0.6862011 ],\n",
      "       [ 0.68400478],\n",
      "       [ 0.68178201],\n",
      "       [ 0.67953384]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run([real_output],feed_dict={X: xd, y: yd}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.01171875,  0.0234375 ,  0.03515625,\n",
       "        0.046875  ,  0.05859375,  0.05859375,  0.0703125 ,  0.08203125,\n",
       "        0.09375   ,  0.10546875,  0.1171875 ,  0.12890625,  0.12890625,\n",
       "        0.140625  ,  0.15234375,  0.1640625 ,  0.17578125,  0.1875    ,\n",
       "        0.19921875,  0.19921875,  0.2109375 ,  0.22265625,  0.234375  ,\n",
       "        0.24609375,  0.2578125 ,  0.26953125,  0.26953125,  0.28125   ,\n",
       "        0.29296875,  0.3046875 ,  0.31640625,  0.328125  ,  0.33984375,\n",
       "        0.33984375,  0.3515625 ,  0.36328125,  0.375     ,  0.38671875,\n",
       "        0.3984375 ,  0.3984375 ,  0.41015625,  0.421875  ,  0.43359375,\n",
       "        0.4453125 ,  0.45703125,  0.46875   ,  0.46875   ,  0.48046875,\n",
       "        0.4921875 ,  0.50390625,  0.515625  ,  0.52734375,  0.5390625 ,\n",
       "        0.5390625 ,  0.55078125,  0.5625    ,  0.57421875,  0.5859375 ,\n",
       "        0.59765625,  0.609375  ,  0.609375  ,  0.62109375,  0.6328125 ,\n",
       "        0.64453125,  0.65625   ,  0.66796875,  0.6796875 ,  0.6796875 ,\n",
       "        0.69140625,  0.703125  ,  0.71484375,  0.7265625 ,  0.73828125,\n",
       "        0.75      ,  0.75      ,  0.76171875,  0.7734375 ,  0.78515625,\n",
       "        0.796875  ,  0.80859375,  0.80859375,  0.8203125 ,  0.83203125,\n",
       "        0.84375   ,  0.85546875,  0.8671875 ,  0.87890625,  0.87890625,\n",
       "        0.890625  ,  0.90234375,  0.9140625 ,  0.92578125,  0.9375    ,\n",
       "        0.94921875,  0.94921875,  0.9609375 ,  0.97265625,  0.984375  ,\n",
       "        0.99609375,  1.0078125 ,  1.01953125,  1.01953125,  1.03125   ,\n",
       "        1.04296875,  1.0546875 ,  1.06640625,  1.078125  ,  1.08984375,\n",
       "        1.08984375,  1.1015625 ,  1.11328125,  1.125     ,  1.13671875,\n",
       "        1.1484375 ,  1.1484375 ,  1.16015625,  1.171875  ,  1.18359375,\n",
       "        1.1953125 ,  1.20703125,  1.21875   ,  1.21875   ,  1.23046875,\n",
       "        1.2421875 ,  1.25390625,  1.265625  ,  1.27734375,  1.2890625 ,\n",
       "        1.2890625 ,  1.30078125,  1.3125    ,  1.32421875,  1.3359375 ,\n",
       "        1.34765625,  1.359375  ,  1.359375  ,  1.37109375,  1.3828125 ,\n",
       "        1.39453125,  1.40625   ,  1.41796875,  1.4296875 ,  1.4296875 ,\n",
       "        1.44140625,  1.453125  ,  1.46484375,  1.4765625 ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.01171875]], dtype=float32)]\n",
      "0.01171875\n"
     ]
    }
   ],
   "source": [
    "BITS = np.array([0,0,0,0,0,0,0,1])\n",
    "tf.reset_default_graph()\n",
    "bits_test = tf.placeholder(tf.float32,shape=[None,8])\n",
    "real_test = tf_bits2real(bits_test, bits=8,radius=1.5)\n",
    "sess = tf.Session()\n",
    "test_out = sess.run([real_test],feed_dict={bits_test:BITS.reshape([1,-1])})\n",
    "print(test_out)\n",
    "val_out = bits2real(BITS)\n",
    "print(val_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None,8],name='Input')\n",
    "y = tf.placeholder(tf.float32, shape=[None,1],name='Correct_output')\n",
    "output = tf_bits2real(X, bits=8,radius=1.5)\n",
    "loss = output - y\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xd_ = np.arange(-1.49, 1.49, 0.01)\n",
    "xd_ = np.arange(-1.49, 1.49, 0.01)\n",
    "# np.random.shuffle(xd)\n",
    "xd = np.zeros([xd_.shape[0],8])\n",
    "yd= np.zeros([xd_.shape[0],1])\n",
    "for i, x in enumerate(xd_):\n",
    "    xd[i,:] = real2bits(x, bits=8, radius=1.5)\n",
    "    yd[i] = bits2real(xd[i,:], bits=8, radius=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "ml = sess.run([mean_loss],feed_dict={X:xd,y:yd})\n",
    "print(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_35]",
   "language": "python",
   "name": "conda-env-ipykernel_35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
